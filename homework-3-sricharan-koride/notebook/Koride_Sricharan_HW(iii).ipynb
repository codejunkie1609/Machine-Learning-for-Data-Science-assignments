{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1608b313-dcef-4fe2-a1cf-931b80199235",
   "metadata": {},
   "source": [
    "Name: Sricharan Koride\n",
    "ID: 2343517466\n",
    "Github id: sricharan-koride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1488b814-e792-4b71-9e23-7eb60acba558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ce814-439d-4bd4-8369-ca620b7b88b1",
   "metadata": {},
   "source": [
    "## 1(b) Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a7936e-7f5f-4a40-a4c7-a357f1fa1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23  \\\n",
      "0         0      32.00       4.85      17.50       3.35      22.50       3.20   \n",
      "1       250      40.50       1.12      14.00       2.24      21.75       1.30   \n",
      "2       500      40.50       2.60      11.33       4.50      18.25       5.31   \n",
      "3       750      34.50       1.50      20.67       2.87      19.00       2.83   \n",
      "4      1000      34.50       1.50      21.25       3.27      18.25       4.38   \n",
      "..      ...        ...        ...        ...        ...        ...        ...   \n",
      "475  118750      36.50       1.50      15.67       9.74      20.50       2.87   \n",
      "476  119000      36.00       0.00      23.25       1.30      17.00       1.63   \n",
      "477  119250      32.33       5.56      15.33       3.77      14.50       4.82   \n",
      "478  119500      32.00       5.87      12.25       2.17      14.50       2.87   \n",
      "479  119750      38.00       2.12      16.50       3.20      11.25       5.80   \n",
      "\n",
      "    activity  \n",
      "0    cycling  \n",
      "1    cycling  \n",
      "2    cycling  \n",
      "3    cycling  \n",
      "4    cycling  \n",
      "..       ...  \n",
      "475  cycling  \n",
      "476  cycling  \n",
      "477  cycling  \n",
      "478  cycling  \n",
      "479  cycling  \n",
      "\n",
      "[1440 rows x 8 columns]\n",
      "       time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23  \\\n",
      "0         0      41.25       1.30       8.50       1.12      20.75       0.83   \n",
      "1       250      44.00       1.41      12.67       2.36      17.25       2.86   \n",
      "2       500      35.25       4.32      14.00       4.55      17.25       3.70   \n",
      "3       750      33.00       2.12      17.00       1.41      15.00       4.55   \n",
      "4      1000      35.25       5.89      16.25       3.03      14.75       5.63   \n",
      "..      ...        ...        ...        ...        ...        ...        ...   \n",
      "475  118750      41.75       1.79      11.00       6.00      16.67       2.49   \n",
      "476  119000      36.33       0.47      16.00       3.16      20.33       1.70   \n",
      "477  119250      31.50       1.50      21.00       0.00      12.25       7.12   \n",
      "478  119500      34.25       6.38      12.67       2.49      15.25       4.21   \n",
      "479  119750      38.33       0.94      15.25       2.17      20.33       1.25   \n",
      "\n",
      "    activity  \n",
      "0    cycling  \n",
      "1    cycling  \n",
      "2    cycling  \n",
      "3    cycling  \n",
      "4    cycling  \n",
      "..       ...  \n",
      "475  cycling  \n",
      "476  cycling  \n",
      "477  cycling  \n",
      "478  cycling  \n",
      "479  cycling  \n",
      "\n",
      "[5760 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"../data/\"\n",
    "combined_train_data = pd.DataFrame()\n",
    "column_names = [\"time\", \"avg_rss12\", \"var_rss12\", \"avg_rss13\", \"var_rss13\", \"avg_rss23\", \"var_rss23\"]\n",
    "train_bending = ['dataset1.csv', 'dataset2.csv']\n",
    "train_non_bending = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n",
    "for folder in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_data = []\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            if (\"bending\" in folder and file in train_bending) or (\"bending\" not in folder and file in train_non_bending):\n",
    "                \n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path, comment='#', names=column_names)\n",
    "                folder_data.append(df)\n",
    "        combined_folder_data = pd.concat(folder_data)\n",
    "        combined_folder_data['activity'] = folder\n",
    "combined_train_data = pd.concat([combined_train_data, combined_folder_data])\n",
    "\n",
    "print(combined_train_data)\n",
    "\n",
    "\n",
    "combined_test_data = pd.DataFrame()\n",
    "column_names = [\"time\", \"avg_rss12\", \"var_rss12\", \"avg_rss13\", \"var_rss13\", \"avg_rss23\", \"var_rss23\"]\n",
    "train_bending = ['dataset1.csv', 'dataset2.csv']\n",
    "train_non_bending = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n",
    "for folder in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_data = []\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            if (\"bending\" in folder and file not in train_bending) or (\"bending\" not in folder and file not in train_non_bending):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path, comment='#', names=column_names)\n",
    "                folder_data.append(df)\n",
    "        combined_folder_data = pd.concat(folder_data)\n",
    "        combined_folder_data['activity'] = folder\n",
    "combined_test_data = pd.concat([combined_test_data, combined_folder_data])\n",
    "print(combined_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b9ded-bec7-40b8-bd23-fb8e7baf5fca",
   "metadata": {},
   "source": [
    "## 1(c)(i) \n",
    "- mean\n",
    "- median\n",
    "- max\n",
    "- min\n",
    "- variance\n",
    "- mode\n",
    "- standard deviation\n",
    "- kurtosis\n",
    "- skew\n",
    "- quantile\n",
    "- auto correlation \n",
    "- correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60181ed6-92ac-4cd8-a10c-a3789bac61af",
   "metadata": {},
   "source": [
    "## 1(c)(ii) Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series in each instance. You are free to normalize/standardize features or use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad8b9c8-64cd-43ec-abe8-0a0d63d74138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    min-1  max-1     mean-1  median-1  Std Dev-1  1st Quartile-1  \\\n",
      "0   37.25  45.00  40.624792     40.50   1.476967           39.25   \n",
      "1   38.00  45.67  42.812812     42.50   1.435550           42.00   \n",
      "2   35.00  47.40  43.954500     44.33   1.558835           43.00   \n",
      "3   33.00  47.75  42.179812     43.50   3.670666           39.15   \n",
      "4   33.00  45.75  41.678063     41.75   2.243490           41.33   \n",
      "..    ...    ...        ...       ...        ...             ...   \n",
      "83  26.50  44.33  36.687292     36.00   3.529404           34.25   \n",
      "84  25.33  45.00  37.114313     36.25   3.710385           34.50   \n",
      "85  26.75  44.75  36.863375     36.33   3.555787           34.50   \n",
      "86  26.25  44.25  36.957458     36.29   3.434863           34.50   \n",
      "87  27.75  44.67  37.144833     36.33   3.758904           34.00   \n",
      "\n",
      "    3rd Quartile-1  min-2  max-2    mean-2  ...  1st Quartile-5  \\\n",
      "0          42.0000    0.0   1.30  0.358604  ...         33.0000   \n",
      "1          43.6700    0.0   1.22  0.372437  ...         32.0000   \n",
      "2          45.0000    0.0   1.70  0.426250  ...         35.3625   \n",
      "3          45.0000    0.0   3.00  0.696042  ...         30.4575   \n",
      "4          42.7500    0.0   2.83  0.535979  ...         28.4575   \n",
      "..             ...    ...    ...       ...  ...             ...   \n",
      "83         39.3725    0.0  12.89  2.973042  ...         14.6700   \n",
      "84         40.2500    0.0  10.84  2.730000  ...         14.7500   \n",
      "85         39.7500    0.0  11.68  2.757312  ...         15.0000   \n",
      "86         40.2500    0.0   8.64  2.420083  ...         14.0000   \n",
      "87         40.5000    0.0  10.76  2.419062  ...         15.0000   \n",
      "\n",
      "    3rd Quartile-5  min-6  max-6    mean-6  median-6  Std Dev-6  \\\n",
      "0            36.00    0.0   1.92  0.570583     0.430   0.582915   \n",
      "1            34.50    0.0   3.11  0.571083     0.430   0.601010   \n",
      "2            36.50    0.0   1.79  0.493292     0.430   0.513506   \n",
      "3            36.33    0.0   2.18  0.613521     0.500   0.524317   \n",
      "4            31.25    0.0   1.79  0.383292     0.430   0.389164   \n",
      "..             ...    ...    ...       ...       ...        ...   \n",
      "83           18.50    0.0   8.19  3.073313     2.690   1.629675   \n",
      "84           18.50    0.0   9.50  3.076354     2.770   1.824534   \n",
      "85           18.67    0.0   8.81  2.773313     2.590   1.569919   \n",
      "86           18.25    0.0   8.34  2.934625     2.525   1.631380   \n",
      "87           18.75    0.0   8.75  2.822437     2.590   1.637183   \n",
      "\n",
      "    1st Quartile-6  3rd Quartile-6    target  \n",
      "0           0.0000          1.3000  bending1  \n",
      "1           0.0000          1.3000  bending1  \n",
      "2           0.0000          0.9400  bending1  \n",
      "3           0.0000          1.0000  bending1  \n",
      "4           0.0000          0.5000  bending1  \n",
      "..             ...             ...       ...  \n",
      "83          1.9125          4.0875   cycling  \n",
      "84          1.7000          4.0375   cycling  \n",
      "85          1.6400          3.6325   cycling  \n",
      "86          1.6600          4.0300   cycling  \n",
      "87          1.5800          3.7400   cycling  \n",
      "\n",
      "[88 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "\n",
    "stat_columns = [f'{stat}-{i}' for i in range(1, 7) for stat in ['min', 'max', 'mean', 'median', 'Std Dev', '1st Quartile', '3rd Quartile']]\n",
    "\n",
    "stat_data = []\n",
    "target_data = []\n",
    "\n",
    "for folder in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        activity_type = folder\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    " \n",
    "            df = pd.read_csv(file_path, comment='#', names=column_names)\n",
    "            stats = df.agg({\n",
    "                column: ['min', 'max', 'mean', 'median', 'std', \n",
    "                         lambda x: x.quantile(0.25),  \n",
    "                         lambda x: x.quantile(0.75)]  \n",
    "                for column in df.columns[1:7]  \n",
    "            }).T.values.flatten() \n",
    "\n",
    "          \n",
    "            stat_data.append(stats)\n",
    "            target_data.append(activity_type)\n",
    "\n",
    "\n",
    "stats_df = pd.DataFrame(stat_data, columns=stat_columns)\n",
    "stats_df['target'] = target_data\n",
    "\n",
    "has_nan = stats_df.isnull().values.any()\n",
    "\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a66cf-0ad0-4680-a0b4-6e8075b3a795",
   "metadata": {},
   "source": [
    "## 1(c)(iii) Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Python’s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec45bfe4-bb7d-4bc2-900b-cbf496cb4031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature  Standard Deviation  lower bound  upper bound\n",
      "0            min-1            9.569975     8.277355    10.760530\n",
      "1            max-1            4.394362     3.365279     5.227018\n",
      "2           mean-1            5.335718     4.714068     5.917418\n",
      "3         median-1            5.440054     4.808393     5.973968\n",
      "4        Std Dev-1            1.772153     1.584003     1.943415\n",
      "5   1st Quartile-1            6.153590     5.557357     6.627924\n",
      "6   3rd Quartile-1            5.138925     4.326686     5.832495\n",
      "7            min-2            0.000000     0.000000     0.000000\n",
      "8            max-2            5.062729     4.650824     5.408790\n",
      "9           mean-2            1.574164     1.393298     1.698165\n",
      "10        median-2            1.412244     1.235645     1.543419\n",
      "11       Std Dev-2            0.884105     0.805336     0.947394\n",
      "12  1st Quartile-2            0.946386     0.835882     1.030235\n",
      "13  3rd Quartile-2            2.125266     1.874978     2.292376\n",
      "14           min-3            2.956462     2.762065     3.096415\n",
      "15           max-3            4.875137     4.194616     5.464626\n",
      "16          mean-3            4.008380     3.422839     4.501657\n",
      "17        median-3            4.036396     3.388299     4.520996\n",
      "18       Std Dev-3            0.946710     0.758963     1.122918\n",
      "19  1st Quartile-3            4.220658     3.621239     4.701263\n",
      "20  3rd Quartile-3            4.171628     3.560986     4.684042\n",
      "21           min-4            0.000000     0.000000     0.000000\n",
      "22           max-4            2.183625     1.977738     2.368640\n",
      "23          mean-4            1.166114     1.081371     1.220282\n",
      "24        median-4            1.145586     1.054952     1.204409\n",
      "25       Std Dev-4            0.458242     0.419743     0.483532\n",
      "26  1st Quartile-4            0.843620     0.776381     0.889220\n",
      "27  3rd Quartile-4            1.552504     1.432935     1.628581\n",
      "28           min-5            6.124001     4.361594     7.479186\n",
      "29           max-5            5.741238     4.766828     6.591223\n",
      "30          mean-5            5.675593     4.384206     6.739541\n",
      "31        median-5            5.813782     4.465133     6.878091\n",
      "32       Std Dev-5            1.024898     0.812177     1.211396\n",
      "33  1st Quartile-5            6.096465     4.789159     7.147019\n",
      "34  3rd Quartile-5            5.531720     4.342476     6.546717\n",
      "35           min-6            0.045838     0.000000     0.078476\n",
      "36           max-6            2.518921     2.238193     2.762185\n",
      "37          mean-6            1.154812     1.069282     1.214847\n",
      "38        median-6            1.086474     0.995936     1.147400\n",
      "39       Std Dev-6            0.517617     0.478446     0.544512\n",
      "40  1st Quartile-6            0.758584     0.692921     0.807193\n",
      "41  3rd Quartile-6            1.523599     1.406069     1.597504\n"
     ]
    }
   ],
   "source": [
    "def bootstrap(data, num_bootstrap, ci_percentile):\n",
    "  \n",
    "    bootstrapped_stds = []\n",
    "    for num in range(num_bootstrap):\n",
    "       \n",
    "        boot_sample = resample(data)\n",
    "\n",
    "        boot_std = np.std(boot_sample, ddof=1)\n",
    "        bootstrapped_stds.append(boot_std)\n",
    "    \n",
    "\n",
    "    lower_bound = np.percentile(bootstrapped_stds, (100 - ci_percentile) / 2)\n",
    "    upper_bound = np.percentile(bootstrapped_stds, 100 - (100 - ci_percentile) / 2)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "columns = stats_df.columns\n",
    "std_results = []\n",
    "lower_bounds = []\n",
    "upper_bounds = []\n",
    "\n",
    "\n",
    "\n",
    "for column in columns[:-1]:\n",
    "   \n",
    "    std = stats_df[column].std()\n",
    "\n",
    "    lower_bound, upper_bound = bootstrap(stats_df[column], 1000, 90)\n",
    "\n",
    "    std_results.append(std)\n",
    "    lower_bounds.append(lower_bound)\n",
    "    upper_bounds.append(upper_bound)\n",
    "\n",
    "confidence_df = pd.DataFrame({\n",
    "    'Feature': columns[:-1],\n",
    "    'Standard Deviation': std_results,\n",
    "    'lower bound': lower_bounds,\n",
    "    'upper bound': upper_bounds\n",
    "})\n",
    "\n",
    "\n",
    "print(confidence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da1a1a-690c-4003-87d9-fe68db0ae6c8",
   "metadata": {},
   "source": [
    "## 1(c)(iv) Use your judgement to select the three most important time-domain features(one option may be min, mean, and max)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c863258-a5e0-496c-adbe-cfb732f8af13",
   "metadata": {},
   "source": [
    "- Mean gives an overall view of the average activity level.\n",
    "- Standard Deviation reflects the variability in activities, crucial for understanding the differences in motion between bending and non-bending.\n",
    "- Max highlights peak movements, which are essential for identifying significant activity events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41312972-0620-4cd3-91a5-1950100f352f",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4\n",
    "a(a) Training RSS: Linear regression is expected to have lower training RSS due to its simplicity and ability to fit linear relationships more accurately. (b) Test RSS: Linear regression is also expected to have lower test RSS as it is less prone to overfitting, leading to better generalization. (c) Training RSS: The outcome depends on how well the true nonlinear relationship can be approximated by a cubic function. (d) Test RSS: Similarly, the test RSS depends on the ability of the models to capture the nonlinearity and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c30311-1f81-4884-ad0e-1ab18c7e45c8",
   "metadata": {},
   "source": [
    "## Sources referred\n",
    "1. https://medium.com/@dreamferus/time-series-feature-extraction-using-pandas-44af6fb5fce9\n",
    "2. https://medium.com/@whystudying/resampling-with-python-bootstrap-50f21866d7c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fceb08-1d6a-448d-869c-548f83c013ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcaf517-4844-4fa1-ba6d-4ad608cf1e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
