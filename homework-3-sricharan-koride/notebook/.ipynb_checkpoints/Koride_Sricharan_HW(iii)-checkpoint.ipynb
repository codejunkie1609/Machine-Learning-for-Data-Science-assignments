{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1608b313-dcef-4fe2-a1cf-931b80199235",
   "metadata": {},
   "source": [
    "Name: Sricharan Koride\n",
    "ID: 2343517466\n",
    "Github id: sricharan-koride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1488b814-e792-4b71-9e23-7eb60acba558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ce814-439d-4bd4-8369-ca620b7b88b1",
   "metadata": {},
   "source": [
    "## 1(b) Keep datasets 1 and 2 in folders bending1 and bending 2, as well as datasets 1, 2, and 3 in other folders as test data and other datasets as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a7936e-7f5f-4a40-a4c7-a357f1fa1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23  \\\n",
      "0         0      32.00       4.85      17.50       3.35      22.50       3.20   \n",
      "1       250      40.50       1.12      14.00       2.24      21.75       1.30   \n",
      "2       500      40.50       2.60      11.33       4.50      18.25       5.31   \n",
      "3       750      34.50       1.50      20.67       2.87      19.00       2.83   \n",
      "4      1000      34.50       1.50      21.25       3.27      18.25       4.38   \n",
      "..      ...        ...        ...        ...        ...        ...        ...   \n",
      "475  118750      36.50       1.50      15.67       9.74      20.50       2.87   \n",
      "476  119000      36.00       0.00      23.25       1.30      17.00       1.63   \n",
      "477  119250      32.33       5.56      15.33       3.77      14.50       4.82   \n",
      "478  119500      32.00       5.87      12.25       2.17      14.50       2.87   \n",
      "479  119750      38.00       2.12      16.50       3.20      11.25       5.80   \n",
      "\n",
      "    activity  \n",
      "0    cycling  \n",
      "1    cycling  \n",
      "2    cycling  \n",
      "3    cycling  \n",
      "4    cycling  \n",
      "..       ...  \n",
      "475  cycling  \n",
      "476  cycling  \n",
      "477  cycling  \n",
      "478  cycling  \n",
      "479  cycling  \n",
      "\n",
      "[1440 rows x 8 columns]\n",
      "       time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  var_rss23  \\\n",
      "0         0      41.25       1.30       8.50       1.12      20.75       0.83   \n",
      "1       250      44.00       1.41      12.67       2.36      17.25       2.86   \n",
      "2       500      35.25       4.32      14.00       4.55      17.25       3.70   \n",
      "3       750      33.00       2.12      17.00       1.41      15.00       4.55   \n",
      "4      1000      35.25       5.89      16.25       3.03      14.75       5.63   \n",
      "..      ...        ...        ...        ...        ...        ...        ...   \n",
      "475  118750      41.75       1.79      11.00       6.00      16.67       2.49   \n",
      "476  119000      36.33       0.47      16.00       3.16      20.33       1.70   \n",
      "477  119250      31.50       1.50      21.00       0.00      12.25       7.12   \n",
      "478  119500      34.25       6.38      12.67       2.49      15.25       4.21   \n",
      "479  119750      38.33       0.94      15.25       2.17      20.33       1.25   \n",
      "\n",
      "    activity  \n",
      "0    cycling  \n",
      "1    cycling  \n",
      "2    cycling  \n",
      "3    cycling  \n",
      "4    cycling  \n",
      "..       ...  \n",
      "475  cycling  \n",
      "476  cycling  \n",
      "477  cycling  \n",
      "478  cycling  \n",
      "479  cycling  \n",
      "\n",
      "[5760 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "root_directory = \"../data/\"\n",
    "combined_train_data = pd.DataFrame()\n",
    "column_names = [\"time\", \"avg_rss12\", \"var_rss12\", \"avg_rss13\", \"var_rss13\", \"avg_rss23\", \"var_rss23\"]\n",
    "train_bending = ['dataset1.csv', 'dataset2.csv']\n",
    "train_non_bending = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n",
    "for folder in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_data = []\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            if (\"bending\" in folder and file in train_bending) or (\"bending\" not in folder and file in train_non_bending):\n",
    "                \n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path, comment='#', names=column_names)\n",
    "                folder_data.append(df)\n",
    "        combined_folder_data = pd.concat(folder_data)\n",
    "        combined_folder_data['activity'] = folder\n",
    "combined_train_data = pd.concat([combined_train_data, combined_folder_data])\n",
    "\n",
    "print(combined_train_data)\n",
    "\n",
    "\n",
    "combined_test_data = pd.DataFrame()\n",
    "column_names = [\"time\", \"avg_rss12\", \"var_rss12\", \"avg_rss13\", \"var_rss13\", \"avg_rss23\", \"var_rss23\"]\n",
    "train_bending = ['dataset1.csv', 'dataset2.csv']\n",
    "train_non_bending = ['dataset1.csv', 'dataset2.csv', 'dataset3.csv']\n",
    "for folder in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_data = []\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            if (\"bending\" in folder and file not in train_bending) or (\"bending\" not in folder and file not in train_non_bending):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path, comment='#', names=column_names)\n",
    "                folder_data.append(df)\n",
    "        combined_folder_data = pd.concat(folder_data)\n",
    "        combined_folder_data['activity'] = folder\n",
    "combined_test_data = pd.concat([combined_test_data, combined_folder_data])\n",
    "print(combined_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b9ded-bec7-40b8-bd23-fb8e7baf5fca",
   "metadata": {},
   "source": [
    "## 1(c)(i) \n",
    "- mean\n",
    "- median\n",
    "- max\n",
    "- min\n",
    "- variance\n",
    "- mode\n",
    "- standard deviation\n",
    "- kurtosis\n",
    "- skew\n",
    "- quantile\n",
    "- auto correlation \n",
    "- correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60181ed6-92ac-4cd8-a10c-a3789bac61af",
   "metadata": {},
   "source": [
    "## 1(c)(ii) Extract the time-domain features minimum, maximum, mean, median, standard deviation, first quartile, and third quartile for all of the 6 time series in each instance. You are free to normalize/standardize features or use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad8b9c8-64cd-43ec-abe8-0a0d63d74138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "\n",
    "stat_columns = [f'{stat}-{i}' for i in range(1, 7) for stat in ['min', 'max', 'mean', 'median', 'Std Dev', '1st Quartile', '3rd Quartile']]\n",
    "\n",
    "stat_data = []\n",
    "target_data = []\n",
    "\n",
    "for folder in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        activity_type = folder\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    " \n",
    "            df = pd.read_csv(file_path, comment='#', names=column_names)\n",
    "            stats = df.agg({\n",
    "                column: ['min', 'max', 'mean', 'median', 'std', \n",
    "                         lambda x: x.quantile(0.25),  \n",
    "                         lambda x: x.quantile(0.75)]  \n",
    "                for column in df.columns[1:7]  \n",
    "            }).T.values.flatten() \n",
    "\n",
    "          \n",
    "            stat_data.append(stats)\n",
    "            target_data.append(activity_type)\n",
    "\n",
    "\n",
    "stats_df = pd.DataFrame(stat_data, columns=stat_columns)\n",
    "stats_df['target'] = target_data\n",
    "\n",
    "has_nan = stats_df.isnull().values.any()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a66cf-0ad0-4680-a0b4-6e8075b3a795",
   "metadata": {},
   "source": [
    "## 1(c)(iii) Estimate the standard deviation of each of the time-domain features you extracted from the data. Then, use Pythonâ€™s bootstrapped or any other method to build a 90% bootsrap confidence interval for the standard deviation of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da1a1a-690c-4003-87d9-fe68db0ae6c8",
   "metadata": {},
   "source": [
    "## 1(c)(iv) Use your judgement to select the three most important time-domain features(one option may be min, mean, and max)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c863258-a5e0-496c-adbe-cfb732f8af13",
   "metadata": {},
   "source": [
    "- Mean gives an overall view of the average activity level.\n",
    "- Standard Deviation reflects the variability in activities, crucial for understanding the differences in motion between bending and non-bending.\n",
    "- Max highlights peak movements, which are essential for identifying significant activity events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41312972-0620-4cd3-91a5-1950100f352f",
   "metadata": {},
   "source": [
    "## 2. ISLR 3.7.4\n",
    "(a) Training RSS: Linear regression is expected to have lower training RSS due to its simplicity and ability to fit linear relationships more accurately. \n",
    "\n",
    "(b) Test RSS: Linear regression is also expected to have lower test RSS as it is less prone to overfitting, leading to better generalization. \n",
    "\n",
    "(c) Training RSS: The outcome depends on how well the true nonlinear relationship can be approximated by a cubic function. \n",
    "\n",
    "(d) Test RSS: Similarly, the test RSS depends on the ability of the models to capture the nonlinearity and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c30311-1f81-4884-ad0e-1ab18c7e45c8",
   "metadata": {},
   "source": [
    "## Sources referred\n",
    "1. https://medium.com/@dreamferus/time-series-feature-extraction-using-pandas-44af6fb5fce9\n",
    "2. https://medium.com/@whystudying/resampling-with-python-bootstrap-50f21866d7c9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fceb08-1d6a-448d-869c-548f83c013ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
